# Onigumo #

## About ##

Onigumo is yet another web-crawler. It ‚Äúcrawls‚Äù websites or webapps, storing their data in a structured form suitable for further machine processing.

## Architecture ##

Onigumo is composed of three sequentially interconnected components:

* [the Operator](#operator),
* [the Downloader](#downloader),
* [the Parser](#parser).

The flowchart below illustrates the flow of data between those parts:

```mermaid
flowchart LR
    subgraph Crawling
        direction BT
        spider_parser(üï∑Ô∏è PARSER)
        spider_operator(üï∑Ô∏è OPERATOR)
        onigumo_downloader[DOWNLOADER]
    end

    start([START])  --> onigumo_feeder[FEEDER]
    onigumo_feeder  -- .raw --> Crawling
    onigumo_feeder  -- .urls --> Crawling
    onigumo_feeder  -- .json --> Crawling
    Crawling  --> spider_materializer(üï∑Ô∏è MATERIALIZER)
    spider_materializer --> done([END])

    spider_operator     -. "<hash>.urls" .-> onigumo_downloader
    onigumo_downloader -. "<hash>.raw"  .->  spider_parser
    spider_parser     -. "<hash>.json" .->  spider_operator
```

```mermaid
flowchart LR
    subgraph "üï∑Ô∏è Spider"
        direction TB
        spider_parser(PARSER)
        spider_operator(OPERATOR)
        spider_materializer(MATERIALIZER)
    end

    subgraph Onigumo
        onigumo_feeder[FEEDER]
        onigumo_downloader[DOWNLOADER]
    end

    onigumo_feeder  -- .json --> spider_operator
    spider_operator ---> spider_materializer
    onigumo_feeder  -- .urls --> onigumo_downloader
    onigumo_feeder  -- .raw --> spider_parser

    spider_parser     -. "<hash>.json" .->  spider_operator
    onigumo_downloader -. "<hash>.raw"  .->  spider_parser
    spider_operator     -. "<hash>.urls" .-> onigumo_downloader
```

### Operator ###

The Operator determines URL addresses for the Downloader. A Spider is responsible for adding the URLs, which it gets from the structured form of the data provided by the Parser.

The Operator‚Äôs job is to:

1. initialize a Spider,
2. extract new URLs from structured data,
3. insert those URLs onto the Downloader queue.

### Downloader ###

The Downloader fetches and saves the contents and metadata from the unprocessed URL addresses.

The Downloader‚Äôs job is to:

1. read URLs for download,
2. check for the already downloaded URLs,
3. fetch the URLs contents along with its metadata,
4. save the downloaded data.

### Parser ###

Zpracov√°v√° data ze sta≈æen√©ho obsahu a metadat do strukturovan√© podoby.

ƒåinnost _parseru_ se skl√°d√° z:

1. kontroly sta≈æen√Ωch URL adres ke zpracov√°n√≠,
2. zpracov√°v√°n√≠ obsahu a metadat sta≈æen√Ωch URL do strukturovan√© podoby,
3. ukl√°d√°n√≠ strukturovan√Ωch dat.

## Aplikace (pavouci) ##

Ze strukturovan√© podoby dat vyt√°hne pot≈ôebn√© informace.

Podstata v√Ωstupn√≠ch dat ƒçi informac√≠ je z√°visl√° na u≈æivatelsk√Ωch pot≈ôeb√°ch a tak√© podobƒõ internetov√©ho obsahu. Je nemo≈æn√© vytvo≈ôit univerz√°ln√≠ho pavouka spl≈àuj√≠c√≠ho v≈°echny po≈æadavky z kombinace obou v√Ω≈°e zm√≠nƒõn√Ωch. Z tohoto d≈Øvodu je nutn√© si napsat vlastn√≠ho pavouka.

### Materializer ###

## Usage ##

## Credits ##

¬© [Glutexo](https://github.com/Glutexo), [nappex](https://github.com/nappex) 2019 ‚Äì 2022

Licenced under the [MIT license](LICENSE.txt).
